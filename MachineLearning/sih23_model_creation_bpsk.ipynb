{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vlhPgtYpbLAy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import keras\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val = []\n",
        "\n",
        "for i in os.listdir():\n",
        "  if \"ldpc\" in i:\n",
        "    print(\"Compiling file :\", i)\n",
        "    conv_0 = pd.read_csv(i, header=None)\n",
        "    for j in range(100):\n",
        "      val.append(conv_0.iloc[:774, j].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTSeg9qyd6Ba",
        "outputId": "ee1892f2-ba8a-427f-9218-101996942c65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling file : ldpc_6snr.csv\n",
            "Compiling file : ldpc_9snr.csv\n",
            "Compiling file : ldpc_5snr.csv\n",
            "Compiling file : ldpc_7snr.csv\n",
            "Compiling file : ldpc_10snr.csv\n",
            "Compiling file : ldpc_2snr.csv\n",
            "Compiling file : ldpc_3snr.csv\n",
            "Compiling file : ldpc_8snr.csv\n",
            "Compiling file : ldpc_4snr.csv\n",
            "Compiling file : ldpc_0snr.csv\n",
            "Compiling file : ldpc_1snr.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_data_conn = []\n",
        "x_data = pd.read_csv('final-data-bpsk.csv', header=None)\n",
        "for i in range(len(x_data)):\n",
        "  X_data_ext = [[x] for x in x_data.iloc[i, :]]\n",
        "  X_data_conn.append(X_data_ext)\n",
        "\n",
        "for i in val:\n",
        "  X_data_conn.append([[x] for x in i])\n",
        "\n",
        "X_data_conn = np.asarray(X_data_conn).astype(np.float32)\n",
        "X_data_conn.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYFTeRwldmN9",
        "outputId": "53626cda-ce47-40c8-e1dc-771bfa46b389"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5296, 774, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_label = pd.read_csv(\"final-variable-bpsk.csv\").values\n",
        "y_label.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0sZhqnYhRCg",
        "outputId": "e76124cc-42c5-4393-8d29-9de8343e08c1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4195, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_label = pd.read_csv(\"final-variable-bpsk.csv\").values\n",
        "Y = []\n",
        "for i in y_label:\n",
        "  Y.append(i.tolist()[0])\n",
        "\n",
        "for i in range(len(val)):\n",
        "  Y.append(4)\n"
      ],
      "metadata": {
        "id": "CJOhjSpQerks"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = X_data_conn[1:]\n",
        "X_data.shape"
      ],
      "metadata": {
        "id": "PA5ibl7v2n-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3646208a-8fa1-4ea0-ae2e-422c04b30b94"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5295, 774, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_data = np.asarray(Y)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGGi7CoEhWpy",
        "outputId": "b208a934-f9b9-4384-9167-896b6a8fb6ad"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. ... 4. 4. 4.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp_layer = keras.layers.Input(shape=(774,1))\n",
        "\n",
        "#conv1-64, 3x1\n",
        "x = keras.layers.Conv1D(filters=128, kernel_size=3)(inp_layer)\n",
        "x_act = keras.layers.Activation(tf.nn.relu)(x)\n",
        "\n",
        "#conv1-64, 3x1\n",
        "x = keras.layers.Conv1D(filters=64, kernel_size=3)(x_act)\n",
        "x_act = keras.layers.Activation(tf.nn.relu)(x)\n",
        "\n",
        "#conv1-128, 3x1\n",
        "x = keras.layers.Conv1D(filters=128, kernel_size=3)(x_act)\n",
        "x_act = keras.layers.Activation(tf.nn.relu)(x)\n",
        "\n",
        "#global average pooling\n",
        "gap = keras.layers.GlobalAveragePooling1D()(x_act)\n",
        "\n",
        "#dense\n",
        "d1 = keras.layers.Dense(128, activation=\"relu\")(gap)\n",
        "d2 = keras.layers.Dense(64, activation=\"relu\")(d1)\n",
        "out_layer = keras.layers.Dense(5, activation=\"sigmoid\")(d2)\n",
        "\n",
        "model_test1 = keras.Model(inp_layer, out_layer)\n",
        "\n",
        "model_test1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OROKFbx4bdPO",
        "outputId": "dfb03964-290d-4f45-dc5a-0f7c42e5c20a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 774, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 772, 128)          512       \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 772, 128)          0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 770, 64)           24640     \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 770, 64)           0         \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 768, 128)          24704     \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 768, 128)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d_3  (None, 128)               0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 74949 (292.77 KB)\n",
            "Trainable params: 74949 (292.77 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_test1.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "LpNPsYwUe8cZ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, shuffle=True, stratify=y_data)"
      ],
      "metadata": {
        "id": "WWKsxClb4zD8"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgfFQD0a5Mkc",
        "outputId": "179860b9-fbaa-4c24-ffcf-b75f0cfef49f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3706, 774, 1), (1589, 774, 1), (3706,), (1589,))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_t, x_v, y_t, y_v = train_test_split(x_test, y_test, test_size=0.5, shuffle=True, stratify=y_test)"
      ],
      "metadata": {
        "id": "6oDCFaAk5VJL"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_t.shape, x_v.shape, y_t.shape, y_v.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR4cWIYI5mez",
        "outputId": "e549cfc9-87e4-4e32-ed11-09a0657d8d6e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((794, 774, 1), (795, 774, 1), (794,), (795,))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=\"best.h5\"),\n",
        "             tf.keras.callbacks.TensorBoard(log_dir=\"logs\")]"
      ],
      "metadata": {
        "id": "aJThoSJU7Cp5"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ03jOqninq-",
        "outputId": "5a262309-fdce-41d1-e77b-f9d8ef70e3b5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3706, 774, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_test1.fit(x_train, y_train, epochs=200, validation_data=(x_v, y_v), callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12g3pYkZjIO8",
        "outputId": "5ea33899-6f7f-4389-bd61-1bda927d6193"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "116/116 [==============================] - 28s 229ms/step - loss: 0.9330 - accuracy: 0.5148 - val_loss: 0.7296 - val_accuracy: 0.5950\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 29s 248ms/step - loss: 0.7182 - accuracy: 0.6039 - val_loss: 0.7297 - val_accuracy: 0.6277\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 25s 215ms/step - loss: 0.7160 - accuracy: 0.6141 - val_loss: 0.7061 - val_accuracy: 0.6277\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.7139 - accuracy: 0.6074 - val_loss: 0.7052 - val_accuracy: 0.5987\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.7103 - accuracy: 0.6104 - val_loss: 0.7034 - val_accuracy: 0.6088\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 27s 232ms/step - loss: 0.7083 - accuracy: 0.6128 - val_loss: 0.6983 - val_accuracy: 0.6264\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 26s 223ms/step - loss: 0.7106 - accuracy: 0.6098 - val_loss: 0.6962 - val_accuracy: 0.6138\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.7019 - accuracy: 0.6257 - val_loss: 0.7192 - val_accuracy: 0.6553\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 26s 223ms/step - loss: 0.7104 - accuracy: 0.6190 - val_loss: 0.6966 - val_accuracy: 0.5962\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 27s 229ms/step - loss: 0.6956 - accuracy: 0.6328 - val_loss: 0.7109 - val_accuracy: 0.6289\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.6889 - accuracy: 0.6425 - val_loss: 0.7135 - val_accuracy: 0.6050\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.6741 - accuracy: 0.6462 - val_loss: 0.6653 - val_accuracy: 0.6855\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.6639 - accuracy: 0.6543 - val_loss: 0.6600 - val_accuracy: 0.6352\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.6570 - accuracy: 0.6619 - val_loss: 0.6441 - val_accuracy: 0.6692\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.6787 - accuracy: 0.6535 - val_loss: 0.6694 - val_accuracy: 0.6377\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 25s 216ms/step - loss: 0.6600 - accuracy: 0.6619 - val_loss: 0.6725 - val_accuracy: 0.6553\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 25s 214ms/step - loss: 0.6488 - accuracy: 0.6735 - val_loss: 0.6353 - val_accuracy: 0.6717\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.6497 - accuracy: 0.6692 - val_loss: 0.6296 - val_accuracy: 0.6956\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.6392 - accuracy: 0.6805 - val_loss: 0.6273 - val_accuracy: 0.6969\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.6452 - accuracy: 0.6754 - val_loss: 0.6338 - val_accuracy: 0.6969\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.6514 - accuracy: 0.6735 - val_loss: 0.6417 - val_accuracy: 0.7107\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 24s 204ms/step - loss: 0.6487 - accuracy: 0.6708 - val_loss: 0.6490 - val_accuracy: 0.6503\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 27s 229ms/step - loss: 0.6348 - accuracy: 0.6835 - val_loss: 0.6223 - val_accuracy: 0.7107\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.6285 - accuracy: 0.6967 - val_loss: 0.6159 - val_accuracy: 0.7170\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 27s 232ms/step - loss: 0.6351 - accuracy: 0.6824 - val_loss: 0.6505 - val_accuracy: 0.6679\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 28s 243ms/step - loss: 0.6360 - accuracy: 0.6905 - val_loss: 0.6143 - val_accuracy: 0.7069\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 26s 228ms/step - loss: 0.6456 - accuracy: 0.6589 - val_loss: 0.6444 - val_accuracy: 0.6830\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.6277 - accuracy: 0.6927 - val_loss: 0.6163 - val_accuracy: 0.7195\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 24s 208ms/step - loss: 0.6248 - accuracy: 0.6913 - val_loss: 0.6067 - val_accuracy: 0.7233\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.6305 - accuracy: 0.6919 - val_loss: 0.6176 - val_accuracy: 0.7119\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.6235 - accuracy: 0.6956 - val_loss: 0.6096 - val_accuracy: 0.7208\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.6401 - accuracy: 0.6937 - val_loss: 0.6642 - val_accuracy: 0.6553\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.6389 - accuracy: 0.6854 - val_loss: 0.6064 - val_accuracy: 0.7233\n",
            "Epoch 34/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.6235 - accuracy: 0.6962 - val_loss: 0.6198 - val_accuracy: 0.6994\n",
            "Epoch 35/200\n",
            "116/116 [==============================] - 26s 219ms/step - loss: 0.6300 - accuracy: 0.6881 - val_loss: 0.6267 - val_accuracy: 0.6906\n",
            "Epoch 36/200\n",
            "116/116 [==============================] - 25s 214ms/step - loss: 0.6171 - accuracy: 0.7016 - val_loss: 0.5907 - val_accuracy: 0.7497\n",
            "Epoch 37/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.6191 - accuracy: 0.6962 - val_loss: 0.6140 - val_accuracy: 0.7371\n",
            "Epoch 38/200\n",
            "116/116 [==============================] - 26s 229ms/step - loss: 0.6092 - accuracy: 0.7086 - val_loss: 0.6014 - val_accuracy: 0.7333\n",
            "Epoch 39/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.6122 - accuracy: 0.7013 - val_loss: 0.6003 - val_accuracy: 0.7333\n",
            "Epoch 40/200\n",
            "116/116 [==============================] - 25s 215ms/step - loss: 0.6058 - accuracy: 0.7156 - val_loss: 0.5914 - val_accuracy: 0.7384\n",
            "Epoch 41/200\n",
            "116/116 [==============================] - 26s 220ms/step - loss: 0.6095 - accuracy: 0.7026 - val_loss: 0.5997 - val_accuracy: 0.7220\n",
            "Epoch 42/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.6087 - accuracy: 0.7137 - val_loss: 0.5950 - val_accuracy: 0.7283\n",
            "Epoch 43/200\n",
            "116/116 [==============================] - 26s 229ms/step - loss: 0.6102 - accuracy: 0.7064 - val_loss: 0.5882 - val_accuracy: 0.7421\n",
            "Epoch 44/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.6087 - accuracy: 0.7094 - val_loss: 0.5907 - val_accuracy: 0.7371\n",
            "Epoch 45/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.6108 - accuracy: 0.7124 - val_loss: 0.5881 - val_accuracy: 0.7421\n",
            "Epoch 46/200\n",
            "116/116 [==============================] - 24s 205ms/step - loss: 0.5984 - accuracy: 0.7159 - val_loss: 0.5833 - val_accuracy: 0.7409\n",
            "Epoch 47/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.6033 - accuracy: 0.7118 - val_loss: 0.5958 - val_accuracy: 0.7421\n",
            "Epoch 48/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.5992 - accuracy: 0.7191 - val_loss: 0.5824 - val_accuracy: 0.7421\n",
            "Epoch 49/200\n",
            "116/116 [==============================] - 29s 250ms/step - loss: 0.6147 - accuracy: 0.7080 - val_loss: 0.5990 - val_accuracy: 0.7484\n",
            "Epoch 50/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.5944 - accuracy: 0.7213 - val_loss: 0.5701 - val_accuracy: 0.7522\n",
            "Epoch 51/200\n",
            "116/116 [==============================] - 26s 223ms/step - loss: 0.5968 - accuracy: 0.7142 - val_loss: 0.5709 - val_accuracy: 0.7560\n",
            "Epoch 52/200\n",
            "116/116 [==============================] - 24s 206ms/step - loss: 0.5923 - accuracy: 0.7221 - val_loss: 0.5711 - val_accuracy: 0.7535\n",
            "Epoch 53/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.5965 - accuracy: 0.7186 - val_loss: 0.5778 - val_accuracy: 0.7434\n",
            "Epoch 54/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.5943 - accuracy: 0.7237 - val_loss: 0.5732 - val_accuracy: 0.7610\n",
            "Epoch 55/200\n",
            "116/116 [==============================] - 26s 228ms/step - loss: 0.5949 - accuracy: 0.7199 - val_loss: 0.5939 - val_accuracy: 0.7283\n",
            "Epoch 56/200\n",
            "116/116 [==============================] - 26s 227ms/step - loss: 0.5815 - accuracy: 0.7291 - val_loss: 0.5659 - val_accuracy: 0.7585\n",
            "Epoch 57/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.5836 - accuracy: 0.7275 - val_loss: 0.5939 - val_accuracy: 0.7119\n",
            "Epoch 58/200\n",
            "116/116 [==============================] - 25s 216ms/step - loss: 0.5860 - accuracy: 0.7264 - val_loss: 0.5702 - val_accuracy: 0.7585\n",
            "Epoch 59/200\n",
            "116/116 [==============================] - 26s 224ms/step - loss: 0.5813 - accuracy: 0.7334 - val_loss: 0.5555 - val_accuracy: 0.7610\n",
            "Epoch 60/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.6019 - accuracy: 0.7113 - val_loss: 0.5860 - val_accuracy: 0.7484\n",
            "Epoch 61/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.5833 - accuracy: 0.7399 - val_loss: 0.5610 - val_accuracy: 0.7648\n",
            "Epoch 62/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.5868 - accuracy: 0.7264 - val_loss: 0.5699 - val_accuracy: 0.7547\n",
            "Epoch 63/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.5790 - accuracy: 0.7315 - val_loss: 0.5542 - val_accuracy: 0.7723\n",
            "Epoch 64/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.5740 - accuracy: 0.7348 - val_loss: 0.5679 - val_accuracy: 0.7472\n",
            "Epoch 65/200\n",
            "116/116 [==============================] - 26s 224ms/step - loss: 0.5737 - accuracy: 0.7412 - val_loss: 0.5855 - val_accuracy: 0.7195\n",
            "Epoch 66/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.5801 - accuracy: 0.7321 - val_loss: 0.5589 - val_accuracy: 0.7547\n",
            "Epoch 67/200\n",
            "116/116 [==============================] - 27s 229ms/step - loss: 0.5713 - accuracy: 0.7439 - val_loss: 0.5457 - val_accuracy: 0.7673\n",
            "Epoch 68/200\n",
            "116/116 [==============================] - 27s 229ms/step - loss: 0.5700 - accuracy: 0.7407 - val_loss: 0.5553 - val_accuracy: 0.7535\n",
            "Epoch 69/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.5577 - accuracy: 0.7431 - val_loss: 0.5361 - val_accuracy: 0.7736\n",
            "Epoch 70/200\n",
            "116/116 [==============================] - 25s 214ms/step - loss: 0.5697 - accuracy: 0.7345 - val_loss: 0.5385 - val_accuracy: 0.7723\n",
            "Epoch 71/200\n",
            "116/116 [==============================] - 25s 213ms/step - loss: 0.5671 - accuracy: 0.7396 - val_loss: 0.5314 - val_accuracy: 0.7811\n",
            "Epoch 72/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.5534 - accuracy: 0.7518 - val_loss: 0.5706 - val_accuracy: 0.7585\n",
            "Epoch 73/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.6440 - accuracy: 0.6940 - val_loss: 0.6456 - val_accuracy: 0.6692\n",
            "Epoch 74/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.6083 - accuracy: 0.7156 - val_loss: 0.5607 - val_accuracy: 0.7522\n",
            "Epoch 75/200\n",
            "116/116 [==============================] - 25s 216ms/step - loss: 0.5765 - accuracy: 0.7318 - val_loss: 0.5452 - val_accuracy: 0.7660\n",
            "Epoch 76/200\n",
            "116/116 [==============================] - 26s 219ms/step - loss: 0.5673 - accuracy: 0.7423 - val_loss: 0.5502 - val_accuracy: 0.7698\n",
            "Epoch 77/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.5652 - accuracy: 0.7428 - val_loss: 0.5839 - val_accuracy: 0.7245\n",
            "Epoch 78/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.5653 - accuracy: 0.7366 - val_loss: 0.5606 - val_accuracy: 0.7497\n",
            "Epoch 79/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.5471 - accuracy: 0.7555 - val_loss: 0.5368 - val_accuracy: 0.7648\n",
            "Epoch 80/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.5505 - accuracy: 0.7523 - val_loss: 0.5425 - val_accuracy: 0.7723\n",
            "Epoch 81/200\n",
            "116/116 [==============================] - 26s 223ms/step - loss: 0.5581 - accuracy: 0.7366 - val_loss: 0.5562 - val_accuracy: 0.7346\n",
            "Epoch 82/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.5518 - accuracy: 0.7493 - val_loss: 0.5248 - val_accuracy: 0.7799\n",
            "Epoch 83/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.5472 - accuracy: 0.7553 - val_loss: 0.5199 - val_accuracy: 0.7736\n",
            "Epoch 84/200\n",
            "116/116 [==============================] - 27s 228ms/step - loss: 0.5444 - accuracy: 0.7523 - val_loss: 0.5493 - val_accuracy: 0.7648\n",
            "Epoch 85/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.5421 - accuracy: 0.7545 - val_loss: 0.5746 - val_accuracy: 0.7270\n",
            "Epoch 86/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.5314 - accuracy: 0.7620 - val_loss: 0.5102 - val_accuracy: 0.7736\n",
            "Epoch 87/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.5376 - accuracy: 0.7625 - val_loss: 0.5038 - val_accuracy: 0.7774\n",
            "Epoch 88/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.5249 - accuracy: 0.7636 - val_loss: 0.5120 - val_accuracy: 0.7862\n",
            "Epoch 89/200\n",
            "116/116 [==============================] - 25s 215ms/step - loss: 0.5341 - accuracy: 0.7598 - val_loss: 0.5045 - val_accuracy: 0.7761\n",
            "Epoch 90/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.5173 - accuracy: 0.7712 - val_loss: 0.5642 - val_accuracy: 0.7346\n",
            "Epoch 91/200\n",
            "116/116 [==============================] - 27s 228ms/step - loss: 0.5315 - accuracy: 0.7615 - val_loss: 0.5077 - val_accuracy: 0.7673\n",
            "Epoch 92/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.5179 - accuracy: 0.7688 - val_loss: 0.4973 - val_accuracy: 0.7862\n",
            "Epoch 93/200\n",
            "116/116 [==============================] - 27s 229ms/step - loss: 0.5074 - accuracy: 0.7709 - val_loss: 0.5157 - val_accuracy: 0.7849\n",
            "Epoch 94/200\n",
            "116/116 [==============================] - 28s 238ms/step - loss: 0.5098 - accuracy: 0.7739 - val_loss: 0.4855 - val_accuracy: 0.7899\n",
            "Epoch 95/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.5001 - accuracy: 0.7831 - val_loss: 0.4937 - val_accuracy: 0.7711\n",
            "Epoch 96/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.4834 - accuracy: 0.7844 - val_loss: 0.4691 - val_accuracy: 0.8013\n",
            "Epoch 97/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.4802 - accuracy: 0.7928 - val_loss: 0.4576 - val_accuracy: 0.7937\n",
            "Epoch 98/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.4760 - accuracy: 0.7890 - val_loss: 0.4796 - val_accuracy: 0.7811\n",
            "Epoch 99/200\n",
            "116/116 [==============================] - 25s 216ms/step - loss: 0.4849 - accuracy: 0.7906 - val_loss: 0.4547 - val_accuracy: 0.7950\n",
            "Epoch 100/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.4803 - accuracy: 0.7876 - val_loss: 0.4634 - val_accuracy: 0.7975\n",
            "Epoch 101/200\n",
            "116/116 [==============================] - 24s 205ms/step - loss: 0.4744 - accuracy: 0.7928 - val_loss: 0.4517 - val_accuracy: 0.7950\n",
            "Epoch 102/200\n",
            "116/116 [==============================] - 26s 226ms/step - loss: 0.4600 - accuracy: 0.7971 - val_loss: 0.4504 - val_accuracy: 0.7950\n",
            "Epoch 103/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.4561 - accuracy: 0.7992 - val_loss: 0.4711 - val_accuracy: 0.8025\n",
            "Epoch 104/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.4544 - accuracy: 0.8003 - val_loss: 0.4892 - val_accuracy: 0.8000\n",
            "Epoch 105/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.4548 - accuracy: 0.8019 - val_loss: 0.4638 - val_accuracy: 0.8176\n",
            "Epoch 106/200\n",
            "116/116 [==============================] - 25s 214ms/step - loss: 0.4547 - accuracy: 0.8046 - val_loss: 0.4345 - val_accuracy: 0.8151\n",
            "Epoch 107/200\n",
            "116/116 [==============================] - 24s 204ms/step - loss: 0.4308 - accuracy: 0.8173 - val_loss: 0.4236 - val_accuracy: 0.8138\n",
            "Epoch 108/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.4472 - accuracy: 0.8103 - val_loss: 0.4360 - val_accuracy: 0.8289\n",
            "Epoch 109/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.4319 - accuracy: 0.8165 - val_loss: 0.4322 - val_accuracy: 0.8189\n",
            "Epoch 110/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.4259 - accuracy: 0.8187 - val_loss: 0.4261 - val_accuracy: 0.8151\n",
            "Epoch 111/200\n",
            "116/116 [==============================] - 25s 220ms/step - loss: 0.4216 - accuracy: 0.8152 - val_loss: 0.4250 - val_accuracy: 0.8088\n",
            "Epoch 112/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.4275 - accuracy: 0.8127 - val_loss: 0.4295 - val_accuracy: 0.8138\n",
            "Epoch 113/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.4193 - accuracy: 0.8179 - val_loss: 0.4114 - val_accuracy: 0.8050\n",
            "Epoch 114/200\n",
            "116/116 [==============================] - 26s 220ms/step - loss: 0.4092 - accuracy: 0.8233 - val_loss: 0.4158 - val_accuracy: 0.8075\n",
            "Epoch 115/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.4099 - accuracy: 0.8206 - val_loss: 0.4476 - val_accuracy: 0.8013\n",
            "Epoch 116/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.4117 - accuracy: 0.8265 - val_loss: 0.4036 - val_accuracy: 0.8113\n",
            "Epoch 117/200\n",
            "116/116 [==============================] - 27s 236ms/step - loss: 0.4080 - accuracy: 0.8249 - val_loss: 0.4043 - val_accuracy: 0.8214\n",
            "Epoch 118/200\n",
            "116/116 [==============================] - 24s 204ms/step - loss: 0.3937 - accuracy: 0.8319 - val_loss: 0.4090 - val_accuracy: 0.8164\n",
            "Epoch 119/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.4071 - accuracy: 0.8260 - val_loss: 0.3986 - val_accuracy: 0.8176\n",
            "Epoch 120/200\n",
            "116/116 [==============================] - 27s 232ms/step - loss: 0.3931 - accuracy: 0.8284 - val_loss: 0.4828 - val_accuracy: 0.7711\n",
            "Epoch 121/200\n",
            "116/116 [==============================] - 26s 228ms/step - loss: 0.3943 - accuracy: 0.8300 - val_loss: 0.4245 - val_accuracy: 0.8164\n",
            "Epoch 122/200\n",
            "116/116 [==============================] - 26s 224ms/step - loss: 0.4077 - accuracy: 0.8265 - val_loss: 0.3946 - val_accuracy: 0.8252\n",
            "Epoch 123/200\n",
            "116/116 [==============================] - 25s 212ms/step - loss: 0.3797 - accuracy: 0.8397 - val_loss: 0.3882 - val_accuracy: 0.8277\n",
            "Epoch 124/200\n",
            "116/116 [==============================] - 24s 209ms/step - loss: 0.3795 - accuracy: 0.8392 - val_loss: 0.4383 - val_accuracy: 0.8063\n",
            "Epoch 125/200\n",
            "116/116 [==============================] - 26s 229ms/step - loss: 0.3750 - accuracy: 0.8408 - val_loss: 0.4136 - val_accuracy: 0.8075\n",
            "Epoch 126/200\n",
            "116/116 [==============================] - 26s 228ms/step - loss: 0.3657 - accuracy: 0.8392 - val_loss: 0.4034 - val_accuracy: 0.8176\n",
            "Epoch 127/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.3698 - accuracy: 0.8381 - val_loss: 0.4032 - val_accuracy: 0.8164\n",
            "Epoch 128/200\n",
            "116/116 [==============================] - 26s 227ms/step - loss: 0.3664 - accuracy: 0.8403 - val_loss: 0.3859 - val_accuracy: 0.8138\n",
            "Epoch 129/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.3691 - accuracy: 0.8405 - val_loss: 0.3760 - val_accuracy: 0.8151\n",
            "Epoch 130/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.3733 - accuracy: 0.8392 - val_loss: 0.3788 - val_accuracy: 0.8126\n",
            "Epoch 131/200\n",
            "116/116 [==============================] - 26s 225ms/step - loss: 0.3798 - accuracy: 0.8400 - val_loss: 0.3992 - val_accuracy: 0.8176\n",
            "Epoch 132/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.3511 - accuracy: 0.8511 - val_loss: 0.3864 - val_accuracy: 0.8189\n",
            "Epoch 133/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.3466 - accuracy: 0.8554 - val_loss: 0.3911 - val_accuracy: 0.8138\n",
            "Epoch 134/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.3501 - accuracy: 0.8489 - val_loss: 0.3895 - val_accuracy: 0.8164\n",
            "Epoch 135/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.3457 - accuracy: 0.8527 - val_loss: 0.3682 - val_accuracy: 0.8239\n",
            "Epoch 136/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.3441 - accuracy: 0.8546 - val_loss: 0.4054 - val_accuracy: 0.8252\n",
            "Epoch 137/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.3482 - accuracy: 0.8538 - val_loss: 0.3790 - val_accuracy: 0.8289\n",
            "Epoch 138/200\n",
            "116/116 [==============================] - 25s 220ms/step - loss: 0.3480 - accuracy: 0.8486 - val_loss: 0.3819 - val_accuracy: 0.8239\n",
            "Epoch 139/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.3453 - accuracy: 0.8497 - val_loss: 0.3577 - val_accuracy: 0.8377\n",
            "Epoch 140/200\n",
            "116/116 [==============================] - 27s 232ms/step - loss: 0.3355 - accuracy: 0.8570 - val_loss: 0.4065 - val_accuracy: 0.8075\n",
            "Epoch 141/200\n",
            "116/116 [==============================] - 24s 208ms/step - loss: 0.3341 - accuracy: 0.8562 - val_loss: 0.3861 - val_accuracy: 0.8088\n",
            "Epoch 142/200\n",
            "116/116 [==============================] - 27s 232ms/step - loss: 0.3302 - accuracy: 0.8602 - val_loss: 0.3533 - val_accuracy: 0.8352\n",
            "Epoch 143/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.3359 - accuracy: 0.8600 - val_loss: 0.3876 - val_accuracy: 0.8302\n",
            "Epoch 144/200\n",
            "116/116 [==============================] - 25s 220ms/step - loss: 0.3363 - accuracy: 0.8608 - val_loss: 0.3885 - val_accuracy: 0.8113\n",
            "Epoch 145/200\n",
            "116/116 [==============================] - 26s 227ms/step - loss: 0.3288 - accuracy: 0.8643 - val_loss: 0.3720 - val_accuracy: 0.8264\n",
            "Epoch 146/200\n",
            "116/116 [==============================] - 25s 218ms/step - loss: 0.3229 - accuracy: 0.8597 - val_loss: 0.3687 - val_accuracy: 0.8327\n",
            "Epoch 147/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.3262 - accuracy: 0.8637 - val_loss: 0.3795 - val_accuracy: 0.8352\n",
            "Epoch 148/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.3189 - accuracy: 0.8640 - val_loss: 0.3497 - val_accuracy: 0.8403\n",
            "Epoch 149/200\n",
            "116/116 [==============================] - 27s 233ms/step - loss: 0.3125 - accuracy: 0.8697 - val_loss: 0.3517 - val_accuracy: 0.8616\n",
            "Epoch 150/200\n",
            "116/116 [==============================] - 27s 233ms/step - loss: 0.3110 - accuracy: 0.8718 - val_loss: 0.3488 - val_accuracy: 0.8428\n",
            "Epoch 151/200\n",
            "116/116 [==============================] - 27s 233ms/step - loss: 0.3206 - accuracy: 0.8697 - val_loss: 0.3454 - val_accuracy: 0.8465\n",
            "Epoch 152/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.3271 - accuracy: 0.8551 - val_loss: 0.3521 - val_accuracy: 0.8516\n",
            "Epoch 153/200\n",
            "116/116 [==============================] - 25s 220ms/step - loss: 0.3190 - accuracy: 0.8640 - val_loss: 0.3769 - val_accuracy: 0.8252\n",
            "Epoch 154/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.3062 - accuracy: 0.8775 - val_loss: 0.3554 - val_accuracy: 0.8478\n",
            "Epoch 155/200\n",
            "116/116 [==============================] - 31s 266ms/step - loss: 0.2983 - accuracy: 0.8756 - val_loss: 0.3553 - val_accuracy: 0.8428\n",
            "Epoch 156/200\n",
            "116/116 [==============================] - 25s 214ms/step - loss: 0.3153 - accuracy: 0.8664 - val_loss: 0.3495 - val_accuracy: 0.8591\n",
            "Epoch 157/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.3010 - accuracy: 0.8748 - val_loss: 0.3582 - val_accuracy: 0.8440\n",
            "Epoch 158/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.3074 - accuracy: 0.8743 - val_loss: 0.3392 - val_accuracy: 0.8528\n",
            "Epoch 159/200\n",
            "116/116 [==============================] - 27s 233ms/step - loss: 0.3056 - accuracy: 0.8764 - val_loss: 0.3722 - val_accuracy: 0.8327\n",
            "Epoch 160/200\n",
            "116/116 [==============================] - 27s 233ms/step - loss: 0.2911 - accuracy: 0.8783 - val_loss: 0.3691 - val_accuracy: 0.8340\n",
            "Epoch 161/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.2806 - accuracy: 0.8864 - val_loss: 0.3359 - val_accuracy: 0.8541\n",
            "Epoch 162/200\n",
            "116/116 [==============================] - 28s 240ms/step - loss: 0.2951 - accuracy: 0.8794 - val_loss: 0.3855 - val_accuracy: 0.8365\n",
            "Epoch 163/200\n",
            "116/116 [==============================] - 25s 211ms/step - loss: 0.2879 - accuracy: 0.8745 - val_loss: 0.3798 - val_accuracy: 0.8390\n",
            "Epoch 164/200\n",
            "116/116 [==============================] - 27s 234ms/step - loss: 0.2889 - accuracy: 0.8786 - val_loss: 0.3846 - val_accuracy: 0.8352\n",
            "Epoch 165/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.2957 - accuracy: 0.8772 - val_loss: 0.3475 - val_accuracy: 0.8642\n",
            "Epoch 166/200\n",
            "116/116 [==============================] - 27s 233ms/step - loss: 0.2763 - accuracy: 0.8888 - val_loss: 0.3687 - val_accuracy: 0.8579\n",
            "Epoch 167/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.2827 - accuracy: 0.8853 - val_loss: 0.3206 - val_accuracy: 0.8717\n",
            "Epoch 168/200\n",
            "116/116 [==============================] - 25s 213ms/step - loss: 0.2803 - accuracy: 0.8902 - val_loss: 0.3205 - val_accuracy: 0.8717\n",
            "Epoch 169/200\n",
            "116/116 [==============================] - 25s 214ms/step - loss: 0.2755 - accuracy: 0.8883 - val_loss: 0.3374 - val_accuracy: 0.8604\n",
            "Epoch 170/200\n",
            "116/116 [==============================] - 26s 223ms/step - loss: 0.2752 - accuracy: 0.8907 - val_loss: 0.3543 - val_accuracy: 0.8591\n",
            "Epoch 171/200\n",
            "116/116 [==============================] - 27s 233ms/step - loss: 0.2812 - accuracy: 0.8907 - val_loss: 0.4031 - val_accuracy: 0.8176\n",
            "Epoch 172/200\n",
            "116/116 [==============================] - 27s 232ms/step - loss: 0.2684 - accuracy: 0.8964 - val_loss: 0.3303 - val_accuracy: 0.8780\n",
            "Epoch 173/200\n",
            "116/116 [==============================] - 26s 227ms/step - loss: 0.2793 - accuracy: 0.8867 - val_loss: 0.3679 - val_accuracy: 0.8654\n",
            "Epoch 174/200\n",
            "116/116 [==============================] - 25s 212ms/step - loss: 0.2647 - accuracy: 0.8972 - val_loss: 0.3375 - val_accuracy: 0.8642\n",
            "Epoch 175/200\n",
            "116/116 [==============================] - 26s 224ms/step - loss: 0.2524 - accuracy: 0.8985 - val_loss: 0.3356 - val_accuracy: 0.8692\n",
            "Epoch 176/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.2662 - accuracy: 0.8934 - val_loss: 0.3174 - val_accuracy: 0.8679\n",
            "Epoch 177/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.2621 - accuracy: 0.8953 - val_loss: 0.3709 - val_accuracy: 0.8428\n",
            "Epoch 178/200\n",
            "116/116 [==============================] - 27s 232ms/step - loss: 0.2666 - accuracy: 0.8950 - val_loss: 0.3232 - val_accuracy: 0.8730\n",
            "Epoch 179/200\n",
            "116/116 [==============================] - 26s 222ms/step - loss: 0.2613 - accuracy: 0.8950 - val_loss: 0.4091 - val_accuracy: 0.8503\n",
            "Epoch 180/200\n",
            "116/116 [==============================] - 27s 230ms/step - loss: 0.2683 - accuracy: 0.8942 - val_loss: 0.3399 - val_accuracy: 0.8730\n",
            "Epoch 181/200\n",
            "116/116 [==============================] - 24s 207ms/step - loss: 0.2502 - accuracy: 0.8991 - val_loss: 0.3269 - val_accuracy: 0.8604\n",
            "Epoch 182/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.2480 - accuracy: 0.9037 - val_loss: 0.3475 - val_accuracy: 0.8604\n",
            "Epoch 183/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.2739 - accuracy: 0.8945 - val_loss: 0.3156 - val_accuracy: 0.8755\n",
            "Epoch 184/200\n",
            "116/116 [==============================] - 30s 256ms/step - loss: 0.2498 - accuracy: 0.9004 - val_loss: 0.3316 - val_accuracy: 0.8604\n",
            "Epoch 185/200\n",
            "116/116 [==============================] - 26s 225ms/step - loss: 0.2546 - accuracy: 0.8985 - val_loss: 0.3339 - val_accuracy: 0.8692\n",
            "Epoch 186/200\n",
            "116/116 [==============================] - 25s 216ms/step - loss: 0.2495 - accuracy: 0.8996 - val_loss: 0.3237 - val_accuracy: 0.8679\n",
            "Epoch 187/200\n",
            "116/116 [==============================] - 26s 225ms/step - loss: 0.2366 - accuracy: 0.9053 - val_loss: 0.3352 - val_accuracy: 0.8528\n",
            "Epoch 188/200\n",
            "116/116 [==============================] - 27s 232ms/step - loss: 0.2536 - accuracy: 0.8999 - val_loss: 0.3576 - val_accuracy: 0.8528\n",
            "Epoch 189/200\n",
            "116/116 [==============================] - 26s 221ms/step - loss: 0.2287 - accuracy: 0.9096 - val_loss: 0.3320 - val_accuracy: 0.8717\n",
            "Epoch 190/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.2431 - accuracy: 0.9058 - val_loss: 0.2962 - val_accuracy: 0.8906\n",
            "Epoch 191/200\n",
            "116/116 [==============================] - 27s 236ms/step - loss: 0.2208 - accuracy: 0.9142 - val_loss: 0.3053 - val_accuracy: 0.8830\n",
            "Epoch 192/200\n",
            "116/116 [==============================] - 26s 223ms/step - loss: 0.2326 - accuracy: 0.9126 - val_loss: 0.3313 - val_accuracy: 0.8717\n",
            "Epoch 193/200\n",
            "116/116 [==============================] - 24s 208ms/step - loss: 0.2322 - accuracy: 0.9088 - val_loss: 0.3152 - val_accuracy: 0.8843\n",
            "Epoch 194/200\n",
            "116/116 [==============================] - 25s 217ms/step - loss: 0.2349 - accuracy: 0.9126 - val_loss: 0.3209 - val_accuracy: 0.8830\n",
            "Epoch 195/200\n",
            "116/116 [==============================] - 27s 231ms/step - loss: 0.2443 - accuracy: 0.9050 - val_loss: 0.2878 - val_accuracy: 0.8906\n",
            "Epoch 196/200\n",
            "116/116 [==============================] - 25s 220ms/step - loss: 0.2294 - accuracy: 0.9074 - val_loss: 0.4028 - val_accuracy: 0.8742\n",
            "Epoch 197/200\n",
            "116/116 [==============================] - 25s 220ms/step - loss: 0.2382 - accuracy: 0.9069 - val_loss: 0.2941 - val_accuracy: 0.8830\n",
            "Epoch 198/200\n",
            "116/116 [==============================] - 25s 219ms/step - loss: 0.2152 - accuracy: 0.9191 - val_loss: 0.3161 - val_accuracy: 0.8792\n",
            "Epoch 199/200\n",
            "116/116 [==============================] - 24s 210ms/step - loss: 0.2231 - accuracy: 0.9150 - val_loss: 0.2766 - val_accuracy: 0.8956\n",
            "Epoch 200/200\n",
            "116/116 [==============================] - 27s 232ms/step - loss: 0.2119 - accuracy: 0.9226 - val_loss: 0.3317 - val_accuracy: 0.8742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d64a551ecb0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_test1.evaluate(x_t, y_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZayzFQS3cDD",
        "outputId": "349a8dc3-e6fb-47d4-cdc0-820777f24746"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 1s 55ms/step - loss: 0.2968 - accuracy: 0.8866\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29680877923965454, 0.8866498470306396]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_test1.save(\"model_ldpc_within_bpsk.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKN-6XmH8KXm",
        "outputId": "0b56ad13-b9cc-4f55-952c-2472017b5ea0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/logs.zip /content/logs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsYtdgyB3l0-",
        "outputId": "97908238-e10a-4cde-b8b9-e3562d91b625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/logs/ (stored 0%)\n",
            "  adding: content/logs/validation/ (stored 0%)\n",
            "  adding: content/logs/validation/events.out.tfevents.1703003851.5a7dddfa4241.624.1.v2 (deflated 79%)\n",
            "  adding: content/logs/train/ (stored 0%)\n",
            "  adding: content/logs/train/events.out.tfevents.1703003833.5a7dddfa4241.624.0.v2 (deflated 86%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.random.randint(0, 2, 774)\n",
        "data = np.array([[x] for x in data])\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JoE08I6Fa79",
        "outputId": "25a38d5b-37de-4436-d23b-71966674e6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(774, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.expand_dims(data, axis=0)\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3UF14DjSI4g",
        "outputId": "cda944a0-e938-4c46-d903-1f2c73756f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 774, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(model_test1.predict(data)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJcyAHVOSU2d",
        "outputId": "e7eaa5d9-5477-4c6e-9def-9b27c28b7f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 39ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_test1.save(\"model\")"
      ],
      "metadata": {
        "id": "3LazKCrdSzSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/model.zip /content/model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMnvFzwRThxR",
        "outputId": "ebc1fc19-704a-4de9-bc3e-d7e6a2cd0c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/model/ (stored 0%)\n",
            "  adding: content/model/fingerprint.pb (stored 0%)\n",
            "  adding: content/model/keras_metadata.pb (deflated 91%)\n",
            "  adding: content/model/saved_model.pb (deflated 88%)\n",
            "  adding: content/model/variables/ (stored 0%)\n",
            "  adding: content/model/variables/variables.index (deflated 65%)\n",
            "  adding: content/model/variables/variables.data-00000-of-00001 (deflated 8%)\n",
            "  adding: content/model/assets/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RNXam4CqTrqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3640fa10-a215-4ef7-8373-4a8d397ae677"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ -6.2288  ],\n",
              "       [-10.852   ],\n",
              "       [ -7.1719  ],\n",
              "       [ -9.5958  ],\n",
              "       [ -6.8879  ],\n",
              "       [  8.1482  ],\n",
              "       [ -7.4452  ],\n",
              "       [-10.363   ],\n",
              "       [ -2.3218  ],\n",
              "       [ 12.854   ],\n",
              "       [  6.0795  ],\n",
              "       [  8.5608  ],\n",
              "       [ -8.6243  ],\n",
              "       [  7.168   ],\n",
              "       [  4.4817  ],\n",
              "       [  5.4672  ],\n",
              "       [-13.134   ],\n",
              "       [ -7.6476  ],\n",
              "       [  9.5916  ],\n",
              "       [  9.3889  ],\n",
              "       [ -8.1082  ],\n",
              "       [ -8.5122  ],\n",
              "       [ -4.7436  ],\n",
              "       [ -6.1975  ],\n",
              "       [ -8.6047  ],\n",
              "       [-10.513   ],\n",
              "       [  3.7734  ],\n",
              "       [ -5.6958  ],\n",
              "       [-13.235   ],\n",
              "       [ -1.1011  ],\n",
              "       [  5.4043  ],\n",
              "       [  3.237   ],\n",
              "       [ -2.2656  ],\n",
              "       [ -8.3731  ],\n",
              "       [  4.7482  ],\n",
              "       [ 17.869   ],\n",
              "       [ -9.0015  ],\n",
              "       [  4.7875  ],\n",
              "       [-10.717   ],\n",
              "       [ -5.0532  ],\n",
              "       [  9.0479  ],\n",
              "       [ -8.2392  ],\n",
              "       [-12.458   ],\n",
              "       [  1.0485  ],\n",
              "       [-11.453   ],\n",
              "       [ -8.8648  ],\n",
              "       [ -9.3906  ],\n",
              "       [  5.4986  ],\n",
              "       [  4.9737  ],\n",
              "       [-10.33    ],\n",
              "       [-11.784   ],\n",
              "       [ -1.8082  ],\n",
              "       [ 10.536   ],\n",
              "       [ -0.70077 ],\n",
              "       [ -2.9622  ],\n",
              "       [ -5.2012  ],\n",
              "       [ -9.3671  ],\n",
              "       [ 10.277   ],\n",
              "       [  9.5136  ],\n",
              "       [  9.6682  ],\n",
              "       [  8.4946  ],\n",
              "       [  5.399   ],\n",
              "       [-11.861   ],\n",
              "       [-17.865   ],\n",
              "       [-12.921   ],\n",
              "       [  8.5331  ],\n",
              "       [-11.465   ],\n",
              "       [ 16.955   ],\n",
              "       [  2.5638  ],\n",
              "       [ 14.189   ],\n",
              "       [  6.1731  ],\n",
              "       [ -8.5159  ],\n",
              "       [  3.0162  ],\n",
              "       [  7.2485  ],\n",
              "       [ 15.917   ],\n",
              "       [ -6.8413  ],\n",
              "       [ 14.43    ],\n",
              "       [ -7.6339  ],\n",
              "       [-10.988   ],\n",
              "       [  6.4739  ],\n",
              "       [-11.38    ],\n",
              "       [ -4.4827  ],\n",
              "       [ 11.759   ],\n",
              "       [  6.3329  ],\n",
              "       [-10.671   ],\n",
              "       [ -3.4454  ],\n",
              "       [  4.497   ],\n",
              "       [ -9.8797  ],\n",
              "       [-13.338   ],\n",
              "       [ -2.3637  ],\n",
              "       [  0.33681 ],\n",
              "       [  4.1042  ],\n",
              "       [  6.0413  ],\n",
              "       [  8.3087  ],\n",
              "       [  5.4205  ],\n",
              "       [ 10.712   ],\n",
              "       [ 10.642   ],\n",
              "       [  6.0704  ],\n",
              "       [-11.261   ],\n",
              "       [ 12.256   ],\n",
              "       [ -5.4037  ],\n",
              "       [  1.6316  ],\n",
              "       [  2.384   ],\n",
              "       [ 11.089   ],\n",
              "       [  5.1929  ],\n",
              "       [ -1.6729  ],\n",
              "       [ 12.248   ],\n",
              "       [ -2.1568  ],\n",
              "       [-15.8     ],\n",
              "       [-17.394   ],\n",
              "       [ -3.7325  ],\n",
              "       [  5.8723  ],\n",
              "       [ -7.9028  ],\n",
              "       [ -7.4046  ],\n",
              "       [  2.6545  ],\n",
              "       [ 14.807   ],\n",
              "       [ 11.488   ],\n",
              "       [  9.5082  ],\n",
              "       [ -4.6252  ],\n",
              "       [-10.194   ],\n",
              "       [ 11.862   ],\n",
              "       [ -4.3363  ],\n",
              "       [ -6.265   ],\n",
              "       [ -5.1288  ],\n",
              "       [  8.9712  ],\n",
              "       [ -9.9503  ],\n",
              "       [ -6.6766  ],\n",
              "       [  6.9752  ],\n",
              "       [  3.8685  ],\n",
              "       [ -3.9667  ],\n",
              "       [ -5.916   ],\n",
              "       [  4.1447  ],\n",
              "       [  6.8025  ],\n",
              "       [  6.5136  ],\n",
              "       [-18.308   ],\n",
              "       [  6.7232  ],\n",
              "       [  9.5046  ],\n",
              "       [ 17.007   ],\n",
              "       [  7.5069  ],\n",
              "       [  9.5318  ],\n",
              "       [ 12.676   ],\n",
              "       [ 10.697   ],\n",
              "       [  7.2718  ],\n",
              "       [-10.3     ],\n",
              "       [-13.292   ],\n",
              "       [ -6.102   ],\n",
              "       [  3.8743  ],\n",
              "       [ 13.312   ],\n",
              "       [-10.1     ],\n",
              "       [ 14.121   ],\n",
              "       [-15.519   ],\n",
              "       [ 11.03    ],\n",
              "       [ -2.8183  ],\n",
              "       [ -5.4368  ],\n",
              "       [ -9.0302  ],\n",
              "       [  2.2832  ],\n",
              "       [  7.5436  ],\n",
              "       [  7.1281  ],\n",
              "       [-14.92    ],\n",
              "       [-10.912   ],\n",
              "       [ -7.5021  ],\n",
              "       [  8.0197  ],\n",
              "       [  0.18956 ],\n",
              "       [ -8.9254  ],\n",
              "       [ -9.3359  ],\n",
              "       [  6.2142  ],\n",
              "       [-13.698   ],\n",
              "       [ -1.0809  ],\n",
              "       [  8.5057  ],\n",
              "       [-16.675   ],\n",
              "       [  8.679   ],\n",
              "       [ 11.878   ],\n",
              "       [  2.7046  ],\n",
              "       [ -4.7054  ],\n",
              "       [  5.223   ],\n",
              "       [  4.3446  ],\n",
              "       [  7.8713  ],\n",
              "       [ 11.034   ],\n",
              "       [  2.4753  ],\n",
              "       [  6.1507  ],\n",
              "       [-11.672   ],\n",
              "       [ -9.4658  ],\n",
              "       [ -9.7707  ],\n",
              "       [  7.7506  ],\n",
              "       [  6.2138  ],\n",
              "       [  0.2881  ],\n",
              "       [-15.143   ],\n",
              "       [ -8.3216  ],\n",
              "       [ 11.939   ],\n",
              "       [-10.902   ],\n",
              "       [ 11.313   ],\n",
              "       [  3.411   ],\n",
              "       [  7.2432  ],\n",
              "       [  8.6069  ],\n",
              "       [ -8.5733  ],\n",
              "       [ -0.968   ],\n",
              "       [  8.9024  ],\n",
              "       [  8.3833  ],\n",
              "       [ -3.6484  ],\n",
              "       [  1.579   ],\n",
              "       [-14.968   ],\n",
              "       [ -4.4811  ],\n",
              "       [  8.0761  ],\n",
              "       [ -8.1101  ],\n",
              "       [-16.634   ],\n",
              "       [  6.9544  ],\n",
              "       [  9.4585  ],\n",
              "       [ -3.8638  ],\n",
              "       [ -8.098   ],\n",
              "       [  5.4538  ],\n",
              "       [  7.1384  ],\n",
              "       [  8.1021  ],\n",
              "       [  3.8356  ],\n",
              "       [ 10.801   ],\n",
              "       [ 11.419   ],\n",
              "       [ -6.5048  ],\n",
              "       [-10.12    ],\n",
              "       [ -6.8968  ],\n",
              "       [ 15.693   ],\n",
              "       [ 13.986   ],\n",
              "       [ -6.5053  ],\n",
              "       [ -7.0726  ],\n",
              "       [ 12.915   ],\n",
              "       [ -9.1898  ],\n",
              "       [  0.55305 ],\n",
              "       [ 11.016   ],\n",
              "       [-10.095   ],\n",
              "       [  9.4613  ],\n",
              "       [ -7.6715  ],\n",
              "       [  0.71127 ],\n",
              "       [  8.2042  ],\n",
              "       [-13.426   ],\n",
              "       [  2.8744  ],\n",
              "       [ -4.8531  ],\n",
              "       [-12.725   ],\n",
              "       [ 11.135   ],\n",
              "       [  2.1571  ],\n",
              "       [ -4.9022  ],\n",
              "       [  7.6377  ],\n",
              "       [ 12.146   ],\n",
              "       [ 11.304   ],\n",
              "       [  6.7125  ],\n",
              "       [-13.24    ],\n",
              "       [ -6.2764  ],\n",
              "       [  8.2604  ],\n",
              "       [ -4.0692  ],\n",
              "       [-11.658   ],\n",
              "       [-11.107   ],\n",
              "       [-11.246   ],\n",
              "       [  8.6397  ],\n",
              "       [ -2.5809  ],\n",
              "       [ -4.7086  ],\n",
              "       [  3.5903  ],\n",
              "       [  6.2135  ],\n",
              "       [ 10.499   ],\n",
              "       [-12.324   ],\n",
              "       [  4.7196  ],\n",
              "       [-16.212   ],\n",
              "       [ -4.0245  ],\n",
              "       [-11.897   ],\n",
              "       [ 10.854   ],\n",
              "       [ -4.3065  ],\n",
              "       [  9.5414  ],\n",
              "       [-10.212   ],\n",
              "       [ -6.5576  ],\n",
              "       [ 10.309   ],\n",
              "       [ -8.6255  ],\n",
              "       [  1.2124  ],\n",
              "       [  4.6613  ],\n",
              "       [ 15.915   ],\n",
              "       [-14.132   ],\n",
              "       [-11.036   ],\n",
              "       [  4.1249  ],\n",
              "       [ -9.8524  ],\n",
              "       [ -1.8207  ],\n",
              "       [ 14.822   ],\n",
              "       [ -6.4768  ],\n",
              "       [  6.3691  ],\n",
              "       [ -8.5726  ],\n",
              "       [  5.7794  ],\n",
              "       [  5.8583  ],\n",
              "       [ -0.15414 ],\n",
              "       [  5.5487  ],\n",
              "       [ 10.08    ],\n",
              "       [-12.822   ],\n",
              "       [  0.47931 ],\n",
              "       [ -4.3719  ],\n",
              "       [ 15.023   ],\n",
              "       [-10.883   ],\n",
              "       [ -9.0984  ],\n",
              "       [-10.423   ],\n",
              "       [  6.1416  ],\n",
              "       [  6.7252  ],\n",
              "       [  4.9525  ],\n",
              "       [ 13.503   ],\n",
              "       [  6.8767  ],\n",
              "       [  2.5438  ],\n",
              "       [ 11.458   ],\n",
              "       [  7.2076  ],\n",
              "       [-10.282   ],\n",
              "       [ -5.3787  ],\n",
              "       [-11.054   ],\n",
              "       [ -4.2543  ],\n",
              "       [ -9.6206  ],\n",
              "       [  6.5412  ],\n",
              "       [ -7.5054  ],\n",
              "       [ 14.359   ],\n",
              "       [  7.9195  ],\n",
              "       [ -7.2132  ],\n",
              "       [ -7.8687  ],\n",
              "       [  5.0417  ],\n",
              "       [ -0.10653 ],\n",
              "       [-11.081   ],\n",
              "       [ -8.2808  ],\n",
              "       [ -6.4116  ],\n",
              "       [-14.289   ],\n",
              "       [ -8.0702  ],\n",
              "       [-13.413   ],\n",
              "       [ 10.32    ],\n",
              "       [  4.6079  ],\n",
              "       [ -5.0114  ],\n",
              "       [  9.4339  ],\n",
              "       [ -8.2585  ],\n",
              "       [  6.945   ],\n",
              "       [-12.856   ],\n",
              "       [-10.599   ],\n",
              "       [  6.2912  ],\n",
              "       [ -8.5664  ],\n",
              "       [-17.265   ],\n",
              "       [ 14.51    ],\n",
              "       [ 10.465   ],\n",
              "       [  0.14539 ],\n",
              "       [  2.3746  ],\n",
              "       [ -4.5453  ],\n",
              "       [  5.0755  ],\n",
              "       [ 13.642   ],\n",
              "       [  5.6231  ],\n",
              "       [  8.7978  ],\n",
              "       [ 11.319   ],\n",
              "       [  7.1296  ],\n",
              "       [ -8.2751  ],\n",
              "       [  6.9228  ],\n",
              "       [ -9.258   ],\n",
              "       [  5.2359  ],\n",
              "       [-11.803   ],\n",
              "       [ -5.5676  ],\n",
              "       [ -5.2431  ],\n",
              "       [  7.4602  ],\n",
              "       [ -4.4781  ],\n",
              "       [ -9.2391  ],\n",
              "       [ -8.5914  ],\n",
              "       [ -4.1431  ],\n",
              "       [  7.4985  ],\n",
              "       [  9.26    ],\n",
              "       [-14.171   ],\n",
              "       [ -6.7285  ],\n",
              "       [ -6.1898  ],\n",
              "       [ -4.2966  ],\n",
              "       [ 14.61    ],\n",
              "       [  7.6719  ],\n",
              "       [ -7.165   ],\n",
              "       [ 10.111   ],\n",
              "       [ -7.6772  ],\n",
              "       [ -3.299   ],\n",
              "       [  6.518   ],\n",
              "       [ -0.044617],\n",
              "       [  7.5292  ],\n",
              "       [  8.828   ],\n",
              "       [  2.0055  ],\n",
              "       [ 11.975   ],\n",
              "       [-10.471   ],\n",
              "       [ 10.936   ],\n",
              "       [-15.183   ],\n",
              "       [  6.4387  ],\n",
              "       [ -9.3576  ],\n",
              "       [ -9.6955  ],\n",
              "       [ -7.9495  ],\n",
              "       [ -6.9565  ],\n",
              "       [ 16.53    ],\n",
              "       [  4.98    ],\n",
              "       [-17.028   ],\n",
              "       [  6.6052  ],\n",
              "       [  9.5036  ],\n",
              "       [  6.8477  ],\n",
              "       [  7.5955  ],\n",
              "       [  8.1958  ],\n",
              "       [ -6.9113  ],\n",
              "       [  7.452   ],\n",
              "       [  2.8386  ],\n",
              "       [ -1.4427  ],\n",
              "       [ 13.89    ],\n",
              "       [ 11.269   ],\n",
              "       [ 10.089   ],\n",
              "       [ -9.3309  ],\n",
              "       [-12.874   ],\n",
              "       [ -5.4909  ],\n",
              "       [  3.7377  ],\n",
              "       [ -5.2185  ],\n",
              "       [ 10.179   ],\n",
              "       [  6.8048  ],\n",
              "       [ -7.9342  ],\n",
              "       [ -8.4596  ],\n",
              "       [ 12.962   ],\n",
              "       [ -3.9068  ],\n",
              "       [  9.2364  ],\n",
              "       [  9.7635  ],\n",
              "       [  3.6332  ],\n",
              "       [ 10.338   ],\n",
              "       [  3.9555  ],\n",
              "       [  9.7323  ],\n",
              "       [-14.332   ],\n",
              "       [-16.385   ],\n",
              "       [ 10.972   ],\n",
              "       [-13.548   ],\n",
              "       [  8.1142  ],\n",
              "       [ -5.164   ],\n",
              "       [  8.4859  ],\n",
              "       [ 13.163   ],\n",
              "       [ 17.72    ],\n",
              "       [ -5.8222  ],\n",
              "       [ -4.4979  ],\n",
              "       [-11.822   ],\n",
              "       [ -5.7241  ],\n",
              "       [ -8.4179  ],\n",
              "       [  0.37725 ],\n",
              "       [  4.4369  ],\n",
              "       [ -5.8249  ],\n",
              "       [ -2.2369  ],\n",
              "       [ -6.8452  ],\n",
              "       [ -8.7076  ],\n",
              "       [ -6.0607  ],\n",
              "       [ 10.054   ],\n",
              "       [ -8.8905  ],\n",
              "       [ -5.8659  ],\n",
              "       [  7.3511  ],\n",
              "       [ 10.595   ],\n",
              "       [  3.8391  ],\n",
              "       [ 15.148   ],\n",
              "       [  8.2256  ],\n",
              "       [-10.753   ],\n",
              "       [  9.1597  ],\n",
              "       [  6.743   ],\n",
              "       [  6.6386  ],\n",
              "       [  8.0915  ],\n",
              "       [ 11.02    ],\n",
              "       [-11.145   ],\n",
              "       [-11.589   ],\n",
              "       [ 12.211   ],\n",
              "       [ -5.872   ],\n",
              "       [ -5.7088  ],\n",
              "       [ 10.485   ],\n",
              "       [ -3.7961  ],\n",
              "       [  7.7727  ],\n",
              "       [ -4.8241  ],\n",
              "       [-13.513   ],\n",
              "       [ -7.9132  ],\n",
              "       [ -2.1291  ],\n",
              "       [ 13.194   ],\n",
              "       [ -6.5558  ],\n",
              "       [  9.6884  ],\n",
              "       [ -8.5146  ],\n",
              "       [ 10.471   ],\n",
              "       [ -8.9699  ],\n",
              "       [-11.361   ],\n",
              "       [ 11.507   ],\n",
              "       [ -7.7776  ],\n",
              "       [  6.945   ],\n",
              "       [  6.0232  ],\n",
              "       [ 10.956   ],\n",
              "       [ -8.5471  ],\n",
              "       [  2.7102  ],\n",
              "       [  2.8041  ],\n",
              "       [ 15.784   ],\n",
              "       [ 14.007   ],\n",
              "       [  7.1691  ],\n",
              "       [  1.793   ],\n",
              "       [-12.372   ],\n",
              "       [  8.2506  ],\n",
              "       [-13.381   ],\n",
              "       [  6.0014  ],\n",
              "       [  5.0492  ],\n",
              "       [ 12.05    ],\n",
              "       [-13.787   ],\n",
              "       [ -6.3731  ],\n",
              "       [  2.0015  ],\n",
              "       [ -4.1149  ],\n",
              "       [ -9.3961  ],\n",
              "       [-10.633   ],\n",
              "       [-12.744   ],\n",
              "       [  4.1369  ],\n",
              "       [ -8.6285  ],\n",
              "       [ -6.9362  ],\n",
              "       [  3.5969  ],\n",
              "       [  8.5999  ],\n",
              "       [-14.666   ],\n",
              "       [ -6.8763  ],\n",
              "       [ 15.811   ],\n",
              "       [  6.069   ],\n",
              "       [ 10.445   ],\n",
              "       [  2.6051  ],\n",
              "       [ -8.4572  ],\n",
              "       [ -9.8019  ],\n",
              "       [ -5.3207  ],\n",
              "       [ -6.8404  ],\n",
              "       [  9.8199  ],\n",
              "       [  7.7165  ],\n",
              "       [ -3.7973  ],\n",
              "       [  8.6274  ],\n",
              "       [ -8.5184  ],\n",
              "       [ -4.3972  ],\n",
              "       [  5.0989  ],\n",
              "       [  2.5271  ],\n",
              "       [ -8.8233  ],\n",
              "       [  4.8357  ],\n",
              "       [ -3.6719  ],\n",
              "       [  1.8846  ],\n",
              "       [ -9.7994  ],\n",
              "       [  9.4532  ],\n",
              "       [ -3.2947  ],\n",
              "       [  8.9289  ],\n",
              "       [ 14.138   ],\n",
              "       [  5.3446  ],\n",
              "       [ -7.4117  ],\n",
              "       [  0.16748 ],\n",
              "       [ 13.508   ],\n",
              "       [ -9.5932  ],\n",
              "       [ -3.695   ],\n",
              "       [  6.3136  ],\n",
              "       [-12.783   ],\n",
              "       [  7.2416  ],\n",
              "       [ -7.7214  ],\n",
              "       [ -7.9088  ],\n",
              "       [  7.2012  ],\n",
              "       [  6.0871  ],\n",
              "       [ 11.278   ],\n",
              "       [ -6.7356  ],\n",
              "       [  1.8451  ],\n",
              "       [  6.3491  ],\n",
              "       [ -6.9216  ],\n",
              "       [ -8.0106  ],\n",
              "       [ -3.5414  ],\n",
              "       [ -0.70962 ],\n",
              "       [ 10.907   ],\n",
              "       [-11.93    ],\n",
              "       [ 10.654   ],\n",
              "       [ -9.5718  ],\n",
              "       [  4.3402  ],\n",
              "       [ -5.7925  ],\n",
              "       [  7.7936  ],\n",
              "       [-10.634   ],\n",
              "       [ -5.0751  ],\n",
              "       [ 11.284   ],\n",
              "       [ -7.6946  ],\n",
              "       [  2.2762  ],\n",
              "       [  2.7906  ],\n",
              "       [  6.921   ],\n",
              "       [ -6.666   ],\n",
              "       [  1.1641  ],\n",
              "       [  2.5431  ],\n",
              "       [  6.5859  ],\n",
              "       [ -8.5736  ],\n",
              "       [  8.5849  ],\n",
              "       [  9.2665  ],\n",
              "       [  8.2303  ],\n",
              "       [ 10.506   ],\n",
              "       [  1.2783  ],\n",
              "       [ 14.668   ],\n",
              "       [  9.4985  ],\n",
              "       [  7.0597  ],\n",
              "       [  5.3251  ],\n",
              "       [ -5.9485  ],\n",
              "       [  4.784   ],\n",
              "       [-11.965   ],\n",
              "       [ -8.859   ],\n",
              "       [-11.779   ],\n",
              "       [  2.2427  ],\n",
              "       [  2.5175  ],\n",
              "       [-15.855   ],\n",
              "       [  6.3528  ],\n",
              "       [ 10.077   ],\n",
              "       [  5.1969  ],\n",
              "       [ -4.5345  ],\n",
              "       [ 11.197   ],\n",
              "       [ -8.1441  ],\n",
              "       [  2.5099  ],\n",
              "       [ -9.9838  ],\n",
              "       [ 10.489   ],\n",
              "       [ -3.6539  ],\n",
              "       [ 11.054   ],\n",
              "       [  5.1497  ],\n",
              "       [ 12.711   ],\n",
              "       [ -0.92812 ],\n",
              "       [ -6.1445  ],\n",
              "       [ 13.836   ],\n",
              "       [ -7.777   ],\n",
              "       [  7.396   ],\n",
              "       [  4.7452  ],\n",
              "       [  7.3719  ],\n",
              "       [  9.9517  ],\n",
              "       [ -8.8401  ],\n",
              "       [ -1.7776  ],\n",
              "       [  8.623   ],\n",
              "       [ 12.543   ],\n",
              "       [ -0.77574 ],\n",
              "       [  7.9798  ],\n",
              "       [  4.0198  ],\n",
              "       [  8.4712  ],\n",
              "       [ 10.706   ],\n",
              "       [ -6.6351  ],\n",
              "       [ 15.576   ],\n",
              "       [-13.492   ],\n",
              "       [  8.9927  ],\n",
              "       [ -5.3343  ],\n",
              "       [ -3.5066  ],\n",
              "       [ -4.8214  ],\n",
              "       [  9.1969  ],\n",
              "       [  4.2393  ],\n",
              "       [ 12.597   ],\n",
              "       [  7.3772  ],\n",
              "       [  9.924   ],\n",
              "       [-16.279   ],\n",
              "       [ -7.7313  ],\n",
              "       [-10.465   ],\n",
              "       [ 10.086   ],\n",
              "       [  1.2641  ],\n",
              "       [ -7.3836  ],\n",
              "       [  6.8137  ],\n",
              "       [-14.613   ],\n",
              "       [  5.9825  ],\n",
              "       [-11.984   ],\n",
              "       [ -7.8913  ],\n",
              "       [ -6.4034  ],\n",
              "       [ -7.2325  ],\n",
              "       [  4.5211  ],\n",
              "       [  8.4709  ],\n",
              "       [  6.6002  ],\n",
              "       [  6.4518  ],\n",
              "       [  9.7142  ],\n",
              "       [  6.1504  ],\n",
              "       [  9.1433  ],\n",
              "       [  3.5013  ],\n",
              "       [  0.67917 ],\n",
              "       [ -7.4346  ],\n",
              "       [-14.501   ],\n",
              "       [-10.865   ],\n",
              "       [-11.142   ],\n",
              "       [ 12.864   ],\n",
              "       [  3.1509  ],\n",
              "       [  0.23457 ],\n",
              "       [ -7.8219  ],\n",
              "       [ -4.1677  ],\n",
              "       [ 13.129   ],\n",
              "       [ -4.2473  ],\n",
              "       [-10.59    ],\n",
              "       [-10.22    ],\n",
              "       [ -6.895   ],\n",
              "       [  7.0642  ],\n",
              "       [ -4.4912  ],\n",
              "       [ -6.1896  ],\n",
              "       [ -6.51    ],\n",
              "       [ -1.6312  ],\n",
              "       [ 16.034   ],\n",
              "       [ -7.5609  ],\n",
              "       [ 12.321   ],\n",
              "       [  8.9933  ],\n",
              "       [  3.5119  ],\n",
              "       [ -9.4519  ],\n",
              "       [ -9.1857  ],\n",
              "       [ -3.0748  ],\n",
              "       [  5.8523  ],\n",
              "       [  6.4686  ],\n",
              "       [ 13.111   ],\n",
              "       [ 17.721   ],\n",
              "       [ -8.4181  ],\n",
              "       [ 10.448   ],\n",
              "       [  7.6322  ],\n",
              "       [ 11.458   ],\n",
              "       [ 10.943   ],\n",
              "       [-13.543   ],\n",
              "       [  6.3977  ],\n",
              "       [ -9.8902  ],\n",
              "       [ 11.909   ],\n",
              "       [ 18.606   ],\n",
              "       [  7.9347  ],\n",
              "       [ -8.5016  ],\n",
              "       [ -3.1612  ],\n",
              "       [ -9.6907  ],\n",
              "       [ 11.37    ],\n",
              "       [  7.3152  ],\n",
              "       [ -9.9084  ],\n",
              "       [  8.6843  ],\n",
              "       [  0.58387 ],\n",
              "       [ -7.33    ],\n",
              "       [ -5.2333  ],\n",
              "       [-12.698   ],\n",
              "       [-11.379   ],\n",
              "       [ -7.4361  ],\n",
              "       [-11.511   ],\n",
              "       [ -2.4205  ],\n",
              "       [  3.6305  ],\n",
              "       [ -1.0567  ],\n",
              "       [ 14.557   ],\n",
              "       [ 11.789   ],\n",
              "       [-11.711   ],\n",
              "       [-12.208   ],\n",
              "       [ -5.6995  ],\n",
              "       [ 11.107   ],\n",
              "       [  6.8693  ],\n",
              "       [ 12.56    ],\n",
              "       [-12.936   ],\n",
              "       [ -9.4202  ],\n",
              "       [  8.5017  ],\n",
              "       [-15.164   ],\n",
              "       [  9.1172  ],\n",
              "       [ -9.6894  ],\n",
              "       [ -1.3586  ],\n",
              "       [ -5.4431  ],\n",
              "       [  9.9338  ],\n",
              "       [  2.2854  ],\n",
              "       [  7.6134  ],\n",
              "       [ -3.3627  ],\n",
              "       [-12.512   ],\n",
              "       [  6.6115  ],\n",
              "       [ -7.2621  ],\n",
              "       [ -9.5508  ],\n",
              "       [ -0.31411 ],\n",
              "       [-10.078   ],\n",
              "       [  8.675   ],\n",
              "       [ 10.672   ],\n",
              "       [  8.8423  ],\n",
              "       [  6.2625  ],\n",
              "       [ 10.995   ],\n",
              "       [-13.163   ],\n",
              "       [ -1.3006  ],\n",
              "       [ -4.3143  ],\n",
              "       [  6.4738  ],\n",
              "       [ 11.401   ],\n",
              "       [-12.45    ],\n",
              "       [-11.304   ],\n",
              "       [ 10.382   ],\n",
              "       [ -8.3148  ],\n",
              "       [  3.4262  ],\n",
              "       [ -4.013   ],\n",
              "       [ -9.1316  ],\n",
              "       [ 14.427   ],\n",
              "       [  6.0768  ],\n",
              "       [  5.7305  ],\n",
              "       [ -3.9798  ],\n",
              "       [-11.014   ],\n",
              "       [  3.431   ],\n",
              "       [  6.0543  ],\n",
              "       [ 10.177   ],\n",
              "       [ 10.7     ],\n",
              "       [ -8.2997  ],\n",
              "       [ 13.588   ],\n",
              "       [ -7.8516  ],\n",
              "       [ 13.12    ],\n",
              "       [ -9.6162  ],\n",
              "       [  4.2809  ],\n",
              "       [ 11.125   ],\n",
              "       [  8.5312  ],\n",
              "       [  4.1525  ],\n",
              "       [ 12.479   ],\n",
              "       [ 12.91    ],\n",
              "       [  6.0114  ],\n",
              "       [  8.0326  ],\n",
              "       [-10.393   ],\n",
              "       [ -3.0769  ],\n",
              "       [-16.531   ],\n",
              "       [ -9.2083  ],\n",
              "       [ 15.257   ],\n",
              "       [ -8.4723  ],\n",
              "       [ -9.0715  ],\n",
              "       [ -7.8173  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QtASNCeFB2ey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}